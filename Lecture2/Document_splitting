#This code is using the langchain library to split a large text into smaller chunks using two different types of text splitters: RecursiveCharacterTextSplitter and CharacterTextSplitter. These are commonly used in Natural Language Processing tasks, especially 
# when preparing data for models with token or character input limits.

from langchain.text_splitter import RecursiveCharacterTextSplitter,CharacterTextSplitter
#chunk_size=26
#Each chunk of text will have up to 26 characters.
#chunk_overlap=4
#Each new chunk will overlap the previous one by 4 characters.


chunk_size=26
chunk_overlap=4
#Smarter splitting strategy.

#It tries to split on natural boundaries like newlines, punctuation, or spaces before falling back to characters.
r_splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunk_size,
    chunk_overlap=chunk_overlap
)
#Simpler, more naive.

#It splits strictly by character length without regard to sentence structure or punctuation.
c_splitter = CharacterTextSplitter(
    chunk_size=chunk_size,
    chunk_overlap=chunk_overlap
)
text1 = 'abcdefghijklmnopqrstuvwxyz'
# by using recursive character splitter
r_splitter.split_text(text1)
c_splitter.split_text(text1)
#'abcdefghijklmnopqrstuvwxyz' → characters 0–25 (length 26)

#'wxyzabcdefg' → starts from index 22 ('w'), and goes to the end (index 33)
text2='abcdefghijklmnopqrstuvwxyzabcdefg'
# by using recursive character splitter
r_splitter.split_text(text2)
# by using character splitter
c_splitter.split_text(text2)
#it have 51 character base on spaces 
#so first 25 is end on m
#then overlap 4 and start this 
#last will shown

text3 = "a b c d e f g h i j k l m n o p q r s t u v w x y z"
# by using recursive character splitter
r_splitter.split_text(text3)
# by using  character splitter
c_splitter.split_text(text3)
#put , and also chunk_size,chunk_overlap
c_splitter = CharacterTextSplitter(
    chunk_size=chunk_size,
    chunk_overlap=chunk_overlap,
    separator = ' '
)
c_splitter.split_text(text3)
#Recursive splitting details
#RecursiveCharacterTextSplitter is recommended for generic text.
some_text="Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Curabitur pretium tincidunt lacus. Nulla."

len(some_text)
#CharacterTextSplitter splits by the single chosen separator (' ')

#RecursiveCharacterTextSplitter tries a list of fallback strategies (paragraphs → lines → words → characters)
c_splitter = CharacterTextSplitter(
    chunk_size=450,
    chunk_overlap=0,
    separator = ' '
)
r_splitter = RecursiveCharacterTextSplitter(
    chunk_size=450,
    chunk_overlap=0, 
    separators=["\n\n", "\n", " ", ""]
)
c_splitter.split_text(some_text)
r_splitter.split_text(some_text)
r_splitter = RecursiveCharacterTextSplitter(
    chunk_size=450,
    chunk_overlap=0, 
    separators=["\n\n", "\n","\.", " ", ""]
)
r_splitter.split_text(some_text)
r_splitter = RecursiveCharacterTextSplitter(
    chunk_size=150,
    chunk_overlap=0,
    separators=["\n\n", "\n", "(?<=\. )", " ", ""]
)
r_splitter.split_text(some_text)
from langchain.document_loaders import PyPDFLoader
#load document
loader= PyPDFLoader(r"F:\Zain Data\All programming language\LangChain\Biology.pdf")
pages=loader.load()
from langchain.text_splitter import CharacterTextSplitter
text_splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=1000,
    chunk_overlap=150,
    length_function=len
)
docs=text_splitter.split_documents(pages)
#it means we can divide 34 pages into small chunks
len(docs)

#34 pages
len(pages)
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import NotionDirectoryLoader

# Load all markdown files from the Notion export folder
loader = NotionDirectoryLoader(r"F:\Zain Data\All programming language\LangChain\notion")
pages = loader.load()

# Split into chunks
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = text_splitter.split_documents(pages)

# Print first chunk
print(docs.page_content[0])


print(docs[0])
print(docs[2])
len(docs)

len(pages)
from langchain.text_splitter import TokenTextSplitter

# Initialize the TokenTextSplitter
text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)

# Input text
text1 = "foo bar bazzyfoo"

# Split the text
chunks = text_splitter.split_text(text1)

# Print the output chunks
print(chunks)

from langchain.schema import Document
from langchain.text_splitter import TokenTextSplitter

# Example documents (pages)
pages = [
    Document(page_content="foo bar bazzyfoo qux quux corge grault garply waldo fred plugh xyzzy thud", metadata={"source": "page1"}),
    Document(page_content="alpha beta gamma delta epsilon zeta eta theta iota kappa lambda", metadata={"source": "page2"})
]

# Initialize the TokenTextSplitter with chunk size of 10 tokens
text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)

# Split the documents into chunks
docs = text_splitter.split_documents(pages)

# Access the first chunk and its metadata
first_chunk = docs[0]
first_metadata = pages[0].metadata

# Output
print(f"First Chunk Content: {first_chunk.page_content}")
print(f"Metadata for First Document: {first_metadata}")

print(pages[0])
pages[0].metadata
#Context aware splitting¶
#Chunking aims to keep text with common context together.

#A text splitting often uses sentences or other delimiters to keep related text together but many documents (such as Markdown) have structure (headers) that can be explicitly used in splitting.

#We can use MarkdownHeaderTextSplitter to preserve header metadata in our chunks, as show below.
from langchain.document_loaders import NotionDirectoryLoader
from langchain.text_splitter import MarkdownHeaderTextSplitter
markdown_document = """# Title\n\n \
## Chapter 1\n\n \
Hi this is Jim\n\n Hi this is Joe\n\n \
### Section \n\n \
Hi this is Lance \n\n 
## Chapter 2\n\n \
Hi this is Molly"""
headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
    ("###", "Header 3"),
]
markdown_splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=headers_to_split_on
)
md_header_splits = markdown_splitter.split_text(markdown_document)
md_header_splits[0]
md_header_splits[1]
md_header_splits[2]
from langchain.document_loaders import NotionDirectoryLoader
from langchain.text_splitter import MarkdownHeaderTextSplitter

# Step 1: Load Notion .md files
loader = NotionDirectoryLoader(r"F:\Zain Data\All programming language\LangChain\notion")
docs = loader.load()

# Step 2: Join the content from all pages
txt = ' '.join([d.page_content for d in docs])

# Step 3: Define headers and initialize splitter
headers_to_split_on = [
    ("#", "Header 1"),
    ("##", "Header 2"),
]
markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)

# Step 4: Split the text by markdown headers
md_header_splits = markdown_splitter.split_text(txt)

# Step 5: Output first split section (optional)
print(md_header_splits[0])

#output optional ogf above use any
md_header_splits[0]
#output optional ogf above use any
md_header_splits[1]
